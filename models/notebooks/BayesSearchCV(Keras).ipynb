{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BayesSearchCV(Keras).ipynb",
      "provenance": [],
      "collapsed_sections": [
        "vYV6AFUDekDn",
        "D_D_iXIBlYFC",
        "j2giFO6IrRNy",
        "-aw2bjghrbQS",
        "Yw9lMBLYlEH6",
        "53FBgh3nBPc-",
        "uyR-pFVyQcme"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vYV6AFUDekDn"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YJuMrSK8em70",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2ce27aae-4db7-435c-e862-4adf64ecb051"
      },
      "source": [
        "from IPython.display import clear_output \n",
        "import pandas as pd\n",
        "import warnings\n",
        "import numpy as np\n",
        "\n",
        "# Scikit-learn\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn import preprocessing\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score, cohen_kappa_score, confusion_matrix, roc_auc_score, roc_curve, auc\n",
        "\n",
        "!pip install scikit-optimize\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "import keras\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "from skopt import BayesSearchCV\n",
        "#Imblearn\n",
        "from imblearn.metrics import geometric_mean_score, classification_report_imbalanced\n",
        "from imblearn.over_sampling import SMOTE, ADASYN, BorderlineSMOTE\n",
        "from imblearn.under_sampling import EditedNearestNeighbours, RepeatedEditedNearestNeighbours, AllKNN, TomekLinks\n",
        "from imblearn.combine import SMOTEENN, SMOTETomek"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting scikit-optimize\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8b/03/be33e89f55866065a02e515c5b319304a801a9f1027a9b311a9b1d1f8dc7/scikit_optimize-0.8.1-py2.py3-none-any.whl (101kB)\n",
            "\r\u001b[K     |███▎                            | 10kB 13.6MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 20kB 10.5MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 30kB 7.2MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 40kB 8.4MB/s eta 0:00:01\r\u001b[K     |████████████████▏               | 51kB 5.4MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 61kB 5.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 71kB 5.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 81kB 6.2MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 92kB 6.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 102kB 3.3MB/s \n",
            "\u001b[?25hCollecting pyaml>=16.9\n",
            "  Downloading https://files.pythonhosted.org/packages/15/c4/1310a054d33abc318426a956e7d6df0df76a6ddfa9c66f6310274fb75d42/pyaml-20.4.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-optimize) (1.0.1)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from scikit-optimize) (1.19.5)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.7/dist-packages (from scikit-optimize) (0.22.2.post1)\n",
            "Requirement already satisfied: scipy>=0.19.1 in /usr/local/lib/python3.7/dist-packages (from scikit-optimize) (1.4.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from pyaml>=16.9->scikit-optimize) (3.13)\n",
            "Installing collected packages: pyaml, scikit-optimize\n",
            "Successfully installed pyaml-20.4.0 scikit-optimize-0.8.1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/externals/six.py:31: FutureWarning: The module is deprecated in version 0.21 and will be removed in version 0.23 since we've dropped support for Python 2.7. Please rely on the official version of six (https://pypi.org/project/six/).\n",
            "  \"(https://pypi.org/project/six/).\", FutureWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.neighbors.base module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.neighbors. Anything that cannot be imported from sklearn.neighbors is now part of the private API.\n",
            "  warnings.warn(message, FutureWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.metrics.classification module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.metrics. Anything that cannot be imported from sklearn.metrics is now part of the private API.\n",
            "  warnings.warn(message, FutureWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D_D_iXIBlYFC"
      },
      "source": [
        "# Auxiliaries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Snc8bSlsl16L"
      },
      "source": [
        "class Resampling:\n",
        "\n",
        "    def __init__(self, name):\n",
        "        self.strategie = None\n",
        "        self.name = name\n",
        "\n",
        "        if name == \"ENN\":\n",
        "            self.strategie = EditedNearestNeighbours(sampling_strategy='auto',\n",
        "                                                     n_neighbors=3,\n",
        "                                                     kind_sel='all',\n",
        "                                                     n_jobs=None)\n",
        "        elif name == \"AllKnn\":\n",
        "            self.strategie = AllKNN(sampling_strategy='auto',\n",
        "                                    n_neighbors=3,\n",
        "                                    kind_sel='all',\n",
        "                                    allow_minority=False,\n",
        "                                    n_jobs=None)\n",
        "        elif name == \"RENN\":\n",
        "            self.strategie = RepeatedEditedNearestNeighbours(sampling_strategy='auto',\n",
        "                                                             n_neighbors=3,\n",
        "                                                             max_iter=100,\n",
        "                                                             kind_sel='all',\n",
        "                                                             n_jobs=None)\n",
        "\n",
        "        elif name == \"TomekLinks\":\n",
        "            self.strategie = TomekLinks(sampling_strategy='auto',  # resample all classes but the minority class;\n",
        "                                        n_jobs=None)\n",
        "\n",
        "        elif name == \"SMOTE\":\n",
        "            self.strategie = SMOTE(sampling_strategy='auto',\n",
        "                                   # equivalent to 'not majority': resample all classes but the majority class;\n",
        "                                   k_neighbors=5,\n",
        "                                   # number of nearest neighbours to used to construct synthetic samples.\n",
        "                                   n_jobs=None,\n",
        "                                   random_state=42)\n",
        "\n",
        "        elif name == \"BorderlineSMOTE\":\n",
        "            self.strategie = BorderlineSMOTE(random_state=42)\n",
        "\n",
        "        elif name == \"ADASYN\":\n",
        "            self.strategie = ADASYN(sampling_strategy='auto',\n",
        "                                    n_neighbors=5,\n",
        "                                    n_jobs=None,\n",
        "                                    random_state=42)\n",
        "\n",
        "        elif name == \"SMOTEENN\":\n",
        "            self.strategie = SMOTEENN(sampling_strategy='auto',\n",
        "                                      smote=None,\n",
        "                                      enn=None,\n",
        "                                      random_state=24)\n",
        "\n",
        "        elif name == \"SMOTETomek\":\n",
        "            self.strategie = SMOTETomek(sampling_strategy='auto',\n",
        "                                        smote=None,\n",
        "                                        tomek=None,\n",
        "                                        random_state=42)\n",
        "\n",
        "    def fit_resample(self, x, y):\n",
        "        x_res, y_res = self.strategie.fit_resample(x, y)\n",
        "        return x_res, y_res"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "toQflDNmli_7"
      },
      "source": [
        "def prepare_data(data, test_size, random_state, resampling=None):\n",
        "    warnings.filterwarnings('ignore')\n",
        "    if resampling is None:\n",
        "        X_train, X_test, Y_train, Y_test = train_test_split(data['RequirementText'], data['Class'],\n",
        "                                                            test_size=test_size,\n",
        "                                                            stratify=data['Class'], random_state=random_state)\n",
        "\n",
        "        tokenizer = Tokenizer()\n",
        "        tokenizer.fit_on_texts(X_train)\n",
        "\n",
        "        x_train = tokenizer.texts_to_matrix(X_train, mode='tfidf')\n",
        "        x_test = tokenizer.texts_to_matrix(X_test, mode='tfidf')\n",
        "\n",
        "        vocab_size = len(tokenizer.word_index) + 1\n",
        "\n",
        "        bin = LabelBinarizer()\n",
        "        bin.fit(Y_train)\n",
        "\n",
        "        y_train = bin.transform(Y_train)\n",
        "        y_test = bin.transform(Y_test)\n",
        "\n",
        "    else:\n",
        "        strategie = Resampling(resampling)\n",
        "        X_train, X_test, Y_train, Y_test = train_test_split(data['RequirementText'], data['Class'],\n",
        "                                                            test_size=test_size,\n",
        "                                                            stratify=data['Class'], random_state=random_state)\n",
        "        tokenizer = Tokenizer()\n",
        "        tokenizer.fit_on_texts(X_train)\n",
        "\n",
        "        x_train = tokenizer.texts_to_matrix(X_train, mode='tfidf')\n",
        "        x_test = tokenizer.texts_to_matrix(X_test, mode='tfidf')\n",
        "\n",
        "        vocab_size = len(tokenizer.word_index) + 1\n",
        "\n",
        "        encoder = LabelBinarizer()\n",
        "        encoder.fit(Y_train)\n",
        "\n",
        "        y_train = encoder.transform(Y_train)\n",
        "        y_test = encoder.transform(Y_test)\n",
        "\n",
        "        x_train, y_train = strategie.fit_resample(x_train, y_train)\n",
        "\n",
        "    return vocab_size, x_train, y_train, x_test, y_test"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BJebYSm8lZSa"
      },
      "source": [
        "class DataSet:\n",
        "    def __init__(self, data):\n",
        "        self.origin = {}\n",
        "        self.tomek = {}\n",
        "        self.smote = {}\n",
        "        self.borderline_smote = {}\n",
        "        self.smote_enn = {}\n",
        "        self.smote_tomek = {}\n",
        "\n",
        "        self.definir_datasets(data)\n",
        "\n",
        "    def definir_datasets(self, data):\n",
        "        # ORIGIN\n",
        "        vocab_size_origin, x_train_origin, y_train_origin, x_test_origin, y_test_origin = prepare_data(data=data,\n",
        "                                                                                                          test_size=0.2,\n",
        "                                                                                                          random_state=42,\n",
        "                                                                                                          resampling=None)\n",
        "        self.set_data('origin', vocab_size_origin, x_train_origin, y_train_origin, x_test_origin, y_test_origin)\n",
        "\n",
        "\n",
        "        # TOMEKLINKS\n",
        "        vocab_size_tomek, x_train_tomek, y_train_tomek, x_test_tomek, y_test_tomek = prepare_data(data=data,\n",
        "                                                                                                     test_size=0.2,\n",
        "                                                                                                     random_state=42,\n",
        "                                                                                                     resampling='TomekLinks')\n",
        "        self.set_data('dataTomek', vocab_size_tomek, x_train_tomek, y_train_tomek, x_test_tomek, y_test_tomek)\n",
        "\n",
        "\n",
        "        # SMOTE\n",
        "        vocab_size_smote, x_train_smote, y_train_smote, x_test_smote, y_test_smote = prepare_data(data=data,\n",
        "                                                                                                     test_size=0.2,\n",
        "                                                                                                     random_state=42,\n",
        "                                                                                                     resampling='SMOTE')\n",
        "        self.set_data('dataSmote', vocab_size_smote, x_train_smote, y_train_smote, x_test_smote, y_test_smote)\n",
        "\n",
        "\n",
        "        # BORDERLINE SMOTE\n",
        "        vocab_size_bd_smote, x_train_bd_smote, y_train_bd_smote, x_test_bd_smote, y_test_bd_smote =prepare_data(\n",
        "            data=data, test_size=0.2, random_state=42,\n",
        "                                         resampling='BorderlineSMOTE')\n",
        "        self.set_data('dataBoderlineSmote', vocab_size_bd_smote, x_train_bd_smote, y_train_bd_smote, x_test_bd_smote,\n",
        "                      y_test_bd_smote)\n",
        "\n",
        "\n",
        "        # SMOTEENN\n",
        "        vocab_size_smoteenn, x_train_smoteenn, y_train_smoteenn, x_test_smoteenn, y_test_smoteenn = prepare_data(\n",
        "            data=data, test_size=0.2, random_state=42, resampling='SMOTEENN')\n",
        "        self.set_data('dataSmoteEnn', vocab_size_smoteenn, x_train_smoteenn, y_train_smoteenn, x_test_smoteenn,\n",
        "                      y_test_smoteenn)\n",
        "\n",
        "\n",
        "        # SMOTETOMEK\n",
        "        vocab_size_smotetomek, x_train_smotetomek, y_train_smotetomek, x_test_smotetomek, y_test_smotetomek= prepare_data(\n",
        "            data=data, test_size=0.2,  random_state=42, resampling='SMOTETomek')\n",
        "        self.set_data('dataSmoteTomek', vocab_size_smotetomek, x_train_smotetomek, y_train_smotetomek, x_test_smotetomek,\n",
        "                      y_test_smotetomek)\n",
        "\n",
        "\n",
        "    def set_data(self, dataname, vocab_size, x_train, y_train, x_test, y_test):\n",
        "        data = {'vocab_size': vocab_size, 'x_train': x_train, 'y_train': y_train, 'x_test': x_test, 'y_test': y_test}\n",
        "        if dataname == 'origin':\n",
        "            self.origin = data\n",
        "        elif dataname == 'dataTomek':\n",
        "            self.tomek = data\n",
        "        elif dataname == 'dataSmote':\n",
        "            self.smote = data\n",
        "        elif dataname == 'dataBoderlineSmote':\n",
        "            self.borderline_smote = data\n",
        "        elif dataname == 'dataSmoteEnn':\n",
        "            self.smote_enn = data\n",
        "        elif dataname == 'dataSmoteTomek':\n",
        "            self.smote_tomek = data\n",
        "\n",
        "    def get_data(self, dataname):\n",
        "        if dataname == 'origin':\n",
        "            return self.origin['vocab_size'], self.origin['x_train'], self.origin['y_train'], self.origin['x_test'],\\\n",
        "                   self.origin['y_test']\n",
        "        elif dataname == 'dataTomek':\n",
        "            return self.tomek['vocab_size'], self.tomek['x_train'], self.tomek['y_train'], self.tomek['x_test'], \\\n",
        "                   self.tomek['y_test']\n",
        "        elif dataname == 'dataSmote':\n",
        "            return self.smote['vocab_size'], self.smote['x_train'], self.smote['y_train'], self.smote['x_test'], \\\n",
        "                   self.smote['y_test']\n",
        "        elif dataname == 'dataBoderlineSmote':\n",
        "            return self.borderline_smote['vocab_size'], self.borderline_smote['x_train'], \\\n",
        "                   self.borderline_smote['y_train'], self.borderline_smote['x_test'], self.borderline_smote['y_test']\n",
        "        elif dataname == 'dataSmoteEnn':\n",
        "            return self.smote_enn['vocab_size'], self.smote_enn['x_train'], self.smote_enn['y_train'], \\\n",
        "                   self.smote_enn['x_test'], self.smote_enn['y_test']\n",
        "        elif dataname == 'dataSmoteTomek':\n",
        "            return self.smote_tomek['vocab_size'], self.smote_tomek['x_train'], self.smote_tomek['y_train'], \\\n",
        "                   self.smote_tomek['x_test'], self.smote_tomek['y_test']\n",
        "\n",
        "    def info_data(self, dataname):\n",
        "        y_train, y_test = 0, 0\n",
        "        if dataname == 'origin':\n",
        "            y_train, y_test = self.origin['y_train'], self.origin['y_test']\n",
        "        elif dataname == 'dataTomek':\n",
        "            y_train, y_test = self.tomek['y_train'], self.tomek['y_test']\n",
        "        elif dataname == 'dataSmote':\n",
        "            y_train, y_test = self.smote['y_train'], self.smote['y_test']\n",
        "        elif dataname == 'dataBorderlineSmote':\n",
        "            y_train, y_test = self.borderline_smote['y_train'], self.borderline_smote['y_test']\n",
        "        elif dataname == 'dataSmoteEnn':\n",
        "            y_train, y_test = self.smote_enn['y_train'], self.smote_enn['y_test']\n",
        "        elif dataname == 'dataSmoteTomek':\n",
        "            y_train, y_test = self.smote_tomek['y_train'], self.smote_tomek['y_test']\n",
        "\n",
        "        bt.plot_requirements_by_class(y_train, y_test)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AYA6Mq6nlBfK"
      },
      "source": [
        "# Hyperparameterization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j2giFO6IrRNy"
      },
      "source": [
        "## BayesSearch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3nq4RjJ2Zph4"
      },
      "source": [
        "def create_net_adamax(input_dim,\n",
        "               nn1,\n",
        "               nn2,\n",
        "               dropout,\n",
        "               l1,\n",
        "               l2,\n",
        "               act,\n",
        "               learn_rate):\n",
        "\n",
        "    loss_fn = 'categorical_crossentropy'\n",
        "\n",
        "    opt = keras.optimizers.Adamax(learning_rate=learn_rate)\n",
        "\n",
        "    reg = keras.regularizers.l1_l2(l1=l1, l2=l2)\n",
        "\n",
        "    model = Sequential()\n",
        "    model.add(Dense(nn1, input_dim=input_dim, activation=act, kernel_regularizer=reg))\n",
        "    model.add(Dropout(dropout))\n",
        "\n",
        "    if nn2 != 0:\n",
        "        model.add(Dense(nn2, activation=act, kernel_regularizer=reg))\n",
        "        model.add(Dropout(dropout))\n",
        "\n",
        "    model.add(Dense(11, activation='softmax'))\n",
        "\n",
        "    model.compile(loss=loss_fn, optimizer=opt, metrics=['accuracy'])\n",
        "\n",
        "    return model\n",
        "\n",
        "def create_net_adam(input_dim,\n",
        "               nn1,\n",
        "               nn2,\n",
        "               dropout,\n",
        "               l1,\n",
        "               l2,\n",
        "               act,\n",
        "               learn_rate):\n",
        "\n",
        "    loss_fn = 'categorical_crossentropy'\n",
        "\n",
        "    opt = keras.optimizers.Adam(learning_rate=learn_rate)\n",
        "\n",
        "    reg = keras.regularizers.l1_l2(l1=l1, l2=l2)\n",
        "\n",
        "    model = Sequential()\n",
        "    model.add(Dense(nn1, input_dim=input_dim, activation=act, kernel_regularizer=reg))\n",
        "    model.add(Dropout(dropout))\n",
        "\n",
        "    if nn2 != 0:\n",
        "        model.add(Dense(nn2, activation=act, kernel_regularizer=reg))\n",
        "        model.add(Dropout(dropout))\n",
        "\n",
        "    model.add(Dense(11, activation='softmax'))\n",
        "\n",
        "    model.compile(loss=loss_fn, optimizer=opt, metrics=['accuracy'])\n",
        "\n",
        "    return model\n",
        "  \n",
        "def create_net_rmsprop(input_dim,\n",
        "               nn1,\n",
        "               nn2,\n",
        "               dropout,\n",
        "               l1,\n",
        "               l2,\n",
        "               act,\n",
        "               learn_rate):\n",
        "\n",
        "    loss_fn = 'categorical_crossentropy'\n",
        "\n",
        "    opt = keras.optimizers.RMSprop(learning_rate=learn_rate)\n",
        "\n",
        "    reg = keras.regularizers.l1_l2(l1=l1, l2=l2)\n",
        "\n",
        "    model = Sequential()\n",
        "    model.add(Dense(nn1, input_dim=input_dim, activation=act, kernel_regularizer=reg))\n",
        "    model.add(Dropout(dropout))\n",
        "\n",
        "    if nn2 != 0:\n",
        "        model.add(Dense(nn2, activation=act, kernel_regularizer=reg))\n",
        "        model.add(Dropout(dropout))\n",
        "\n",
        "    model.add(Dense(11, activation='softmax'))\n",
        "\n",
        "    model.compile(loss=loss_fn, optimizer=opt, metrics=['accuracy'])\n",
        "\n",
        "    return model\n",
        "\n",
        "def create_net_sgdm(input_dim,\n",
        "                    nn1,\n",
        "                    nn2,\n",
        "                    dropout,\n",
        "                    l1,\n",
        "                    l2,\n",
        "                    act,\n",
        "                    learn_rate):\n",
        "\n",
        "    loss_fn = 'categorical_crossentropy'\n",
        "\n",
        "    opt = keras.optimizers.SGD(learning_rate=learn_rate, momentum=0.9)\n",
        "\n",
        "    reg = keras.regularizers.l1_l2(l1=l1, l2=l2)\n",
        "\n",
        "    model = Sequential()\n",
        "    model.add(Dense(nn1, input_dim=input_dim, activation=act, kernel_regularizer=reg))\n",
        "    model.add(Dropout(dropout))\n",
        "\n",
        "    if nn2 != 0:\n",
        "        model.add(Dense(nn2, activation=act, kernel_regularizer=reg))\n",
        "        model.add(Dropout(dropout))\n",
        "\n",
        "    model.add(Dense(11, activation='softmax'))\n",
        "\n",
        "    model.compile(loss=loss_fn, optimizer=opt, metrics=['accuracy'])\n",
        "\n",
        "    return model"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yh29v-BjkzoS"
      },
      "source": [
        "def bayesSearchCV(dataset, optimizer, resampling):\n",
        "  if optimizer=='sgdm':\n",
        "    model = KerasClassifier(build_fn=create_net_sgdm, verbose=0)\n",
        "  elif optimizer=='adamax':\n",
        "    model = KerasClassifier(build_fn=create_net_adamax, verbose=0)\n",
        "  elif optimizer=='adam':\n",
        "    model = KerasClassifier(build_fn=create_net_adam, verbose=0)\n",
        "  elif optimizer=='rmsprop':\n",
        "    model = KerasClassifier(build_fn=create_net_rmsprop, verbose=0)\n",
        "  \n",
        "  vocab_size, x_train, y_train, x_test, y_test = dataset.get_data(resampling)\n",
        "\n",
        "  input_size = [vocab_size]\n",
        "  nn1 = [20, 50, 100, 150, 200]\n",
        "  nn2 = [0, 20, 50, 100, 150, 200]\n",
        "  batch_size = [10, 20, 30, 40, None]\n",
        "  dropout = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5]\n",
        "  l1 = [0.0, 0.01, 0.001, 0.0001]\n",
        "  l2 = [0.0, 0.01, 0.001, 0.0001]\n",
        "  act = ['tanh', 'relu', 'sigmoid', 'elu']\n",
        "  learn_rate = [0.01, 0.001, 0.0001]\n",
        "  epochs = [30, 40, 50, 100]\n",
        "\n",
        "  params = dict(input_dim=input_size, nn1=nn1, nn2=nn2, dropout=dropout,\n",
        "                  l1=l1, l2=l2, act=act, learn_rate=learn_rate, \n",
        "                  batch_size=batch_size, epochs=epochs)\n",
        "  \n",
        "  search = BayesSearchCV(estimator=model, search_spaces=params, cv=10, \n",
        "                          verbose=0, iid=False, n_iter=50, scoring='balanced_accuracy')\n",
        "  search.fit(x_train, np.argmax(y_train, axis=1))\n",
        "\n",
        "  clear_output()\n",
        "  print(search.best_score_)\n",
        "  print(search.best_params_)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-aw2bjghrbQS"
      },
      "source": [
        "## Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oRlU4LbrZrkB"
      },
      "source": [
        "dataset = DataSet(pd.read_csv('PROMISE_exp_preprocessed.csv'))"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yw9lMBLYlEH6"
      },
      "source": [
        "## Origin"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EBjQ0sjc8qEO",
        "outputId": "6627e7a6-de8d-4855-e0b5-510701723e38"
      },
      "source": [
        "bayesSearchCV(dataset, 'adamax', 'origin')"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.6180248917748917\n",
            "OrderedDict([('act', 'relu'), ('batch_size', 30), ('dropout', 0.4), ('epochs', 50), ('input_dim', 1060), ('l1', 0.0), ('l2', 0.0001), ('learn_rate', 0.01), ('nn1', 150), ('nn2', 150)])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9YIMw5C21PIF",
        "outputId": "e1b96696-04d6-4eb1-c0d6-127bb44ad077"
      },
      "source": [
        "bayesSearchCV(dataset, 'sgdm', 'origin')"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.6098875661375661\n",
            "OrderedDict([('act', 'elu'), ('batch_size', 30), ('dropout', 0.5), ('epochs', 40), ('input_dim', 1060), ('l1', 0.0), ('l2', 0.0001), ('learn_rate', 0.01), ('nn1', 100), ('nn2', 150)])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AxHgsWoXrp-6",
        "outputId": "5b7858d5-62e2-42f5-b749-ec5059bc4d7a"
      },
      "source": [
        "bayesSearchCV(dataset, 'rmsprop', 'origin')"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.6207296777296777\n",
            "OrderedDict([('act', 'relu'), ('batch_size', 40), ('dropout', 0.4), ('epochs', 40), ('input_dim', 1060), ('l1', 0.0001), ('l2', 0.0001), ('learn_rate', 0.01), ('nn1', 50), ('nn2', 100)])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "53FBgh3nBPc-"
      },
      "source": [
        "## Tomek"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DNMcL3HgBQ3t",
        "outputId": "28550e51-5a21-461a-abef-eb040503e05c"
      },
      "source": [
        "bayesSearchCV(dataset, 'adamax', 'dataTomek')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.580396762253194\n",
            "OrderedDict([('act', 'relu'), ('batch_size', 20), ('dropout', 0.3), ('epochs', 50), ('input_dim', 1060), ('l1', 0.0), ('l2', 0.0001), ('learn_rate', 0.01), ('nn1', 100), ('nn2', 150)])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5_56dg5yBaEt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "137e05c7-e630-41e4-c8bd-e313fef94083"
      },
      "source": [
        "bayesSearchCV(dataset, 'rmsprop', 'dataTomek')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.5560799284595965\n",
            "OrderedDict([('act', 'sigmoid'), ('batch_size', 10), ('dropout', 0.1), ('epochs', 30), ('input_dim', 1060), ('l1', 0.0), ('l2', 0.0001), ('learn_rate', 0.001), ('nn1', 100), ('nn2', 200)])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uyR-pFVyQcme"
      },
      "source": [
        "## Smote"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "brRyzYjCQ_J9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d88fdb8d-3eb6-4524-94f8-c407505f3dd5"
      },
      "source": [
        "bayesSearchCV(dataset, 'adamax', 'dataSmote')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.9517358139031978\n",
            "OrderedDict([('act', 'relu'), ('batch_size', 10), ('dropout', 0.1), ('epochs', 50), ('input_dim', 1060), ('l1', 0.0), ('l2', 0.0001), ('learn_rate', 0.001), ('nn1', 150), ('nn2', 50)])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FVxBukfqRBcO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d4fe2b1d-1975-4713-e168-fc1891643ae2"
      },
      "source": [
        "bayesSearchCV(dataset, 'rmsprop', 'dataSmote')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.9507271338435508\n",
            "OrderedDict([('act', 'relu'), ('batch_size', 10), ('dropout', 0.2), ('epochs', 30), ('input_dim', 1060), ('l1', 0.0), ('l2', 0.0), ('learn_rate', 0.01), ('nn1', 100), ('nn2', 50)])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WafXrwwv8RcK"
      },
      "source": [
        "## BoderlineSmote"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AgK10X0V8RcW",
        "outputId": "28d308fd-d5cf-4d79-c644-dda161081718"
      },
      "source": [
        "bayesSearchCV(dataset, 'adamax', 'dataBoderlineSmote')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.9014634210857222\n",
            "OrderedDict([('act', 'elu'), ('batch_size', 40), ('dropout', 0.2), ('epochs', 50), ('input_dim', 1060), ('l1', 0.0), ('l2', 0.0001), ('learn_rate', 0.01), ('nn1', 200), ('nn2', 100)])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ctRIJiyu8RcY",
        "outputId": "a3967594-1f71-4fd8-93bf-1eae768cb5e6"
      },
      "source": [
        "bayesSearchCV(dataset, 'rmsprop', 'dataBoderlineSmote')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.9030573464536558\n",
            "OrderedDict([('act', 'tanh'), ('batch_size', 40), ('dropout', 0.3), ('epochs', 40), ('input_dim', 1060), ('l1', 0.0001), ('l2', 0.0), ('learn_rate', 0.001), ('nn1', 200), ('nn2', 50)])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GpQLFebzbNWA"
      },
      "source": [
        "## SmoteEnn"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3gdyT05jbNWD"
      },
      "source": [
        "bayesSearchCV(dataset, 'adamax', 'dataSmoteEnn')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wYN4_EQNbNWE"
      },
      "source": [
        "bayesSearchCV(dataset, 'rmsprop', 'dataSmoteEnn')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qzH9ggo0gtNp"
      },
      "source": [
        "## SmoteTomek"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n0TFxWj2gtNr",
        "outputId": "f7a3d472-b4c5-4733-f69e-e394ee512540"
      },
      "source": [
        "bayesSearchCV(dataset, 'adamax', 'dataSmoteTomek')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.9540489177746023\n",
            "OrderedDict([('act', 'relu'), ('batch_size', 40), ('dropout', 0.0), ('epochs', 30), ('input_dim', 1060), ('l1', 0.0), ('l2', 0.0), ('learn_rate', 0.01), ('nn1', 200), ('nn2', 100)])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gccHlt8WgtNr",
        "outputId": "38e0790f-1fc1-4669-8e21-5922839a1fe1"
      },
      "source": [
        "bayesSearchCV(dataset, 'rmsprop', 'dataSmoteTomek')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.9465133208565298\n",
            "OrderedDict([('act', 'relu'), ('batch_size', 30), ('dropout', 0.0), ('epochs', 40), ('input_dim', 1060), ('l1', 0.0001), ('l2', 0.0001), ('learn_rate', 0.001), ('nn1', 150), ('nn2', 150)])\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}